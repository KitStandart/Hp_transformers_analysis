{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fANJewZfDUG8",
        "outputId": "8c1342d6-089f-4976-fbb0-4fbd5fcc9232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.23.5)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.1.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2023.11.17)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.61.0)\n",
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install keras-nlp keras-core tensorflow-text --no-deps\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4XrU1A5BH9h",
        "outputId": "70e3a9b7-05c3-4942-d2fc-ede4431b5c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "tf.config.list_physical_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H0JlvoTO6MD"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 256\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "ENG_VOCAB_SIZE=VOCAB_SIZE\n",
        "EMBED_DIM=256\n",
        "INTERMEDIATE_DIM=512\n",
        "NUM_HEADS=4\n",
        "SPA_VOCAB_SIZE=VOCAB_SIZE\n",
        "NUM_LAYERS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBAGVbheDJcu"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"ted_hrlr_translate/pt_to_en\"\n",
        "dataset, info = tfds.load(name=dataset_name, with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVXUZalIEOgD"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAbQG16zL_V7"
      },
      "outputs": [],
      "source": [
        "proto = keras_nlp.tokenizers.compute_sentence_piece_proto(train_dataset.map(lambda x, y: x+y), VOCAB_SIZE, model_type=\"bpe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lslpr4UaIpTe"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras_nlp.tokenizers.SentencePieceTokenizer(proto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0uPGOvqVHbb",
        "outputId": "e309ae4c-c028-4531-854c-85c135a4f070"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.detokenize(tokenizer.tokenize(next(iter(train_dataset))[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ocx_mj1afQi",
        "outputId": "0a713850-12ef-4402-8979-096816f52e0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19978"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer._sentence_piece.string_to_id('0').numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iry12l0KCMe"
      },
      "outputs": [],
      "source": [
        "pad_token = tokenizer.token_to_id(\"<pad>\")\n",
        "start_token = tokenizer.token_to_id(\"<s>\")\n",
        "end_token = tokenizer.token_to_id(\"</s>\")\n",
        "\n",
        "def preprocess_batch(eng, spa):\n",
        "    eng = tokenizer(eng)\n",
        "    spa = tokenizer(spa)\n",
        "\n",
        "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
        "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=pad_token,\n",
        "    )\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n",
        "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=start_token,\n",
        "        end_value=end_token,\n",
        "        pad_value=pad_token,\n",
        "    )\n",
        "    spa = spa_start_end_packer(spa)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(dataset):\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_dataset)\n",
        "val_ds = make_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fuJcmnklkBg",
        "outputId": "abc9c7a2-d14a-4931-d37c-ee06a9231b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " token_and_position_embeddi  (None, None, 256)            5145600   ['encoder_inputs[0][0]']      \n",
            " ng (TokenAndPositionEmbedd                                                                       \n",
            " ing)                                                                                             \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            527104    ['token_and_position_embedding\n",
            " formerEncoder)                                                     [0][0]']                      \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Tra  (None, None, 256)            527104    ['transformer_encoder[0][0]'] \n",
            " nsformerEncoder)                                                                                 \n",
            "                                                                                                  \n",
            " transformer_encoder_2 (Tra  (None, None, 256)            527104    ['transformer_encoder_1[0][0]'\n",
            " nsformerEncoder)                                                   ]                             \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " transformer_encoder_3 (Tra  (None, None, 256)            527104    ['transformer_encoder_2[0][0]'\n",
            " nsformerEncoder)                                                   ]                             \n",
            "                                                                                                  \n",
            " model_1 (Functional)        (None, None, 20000)          1344873   ['decoder_inputs[0][0]',      \n",
            "                                                          6          'transformer_encoder_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20702752 (78.97 MB)\n",
            "Trainable params: 20702752 (78.97 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "for layer in range(NUM_LAYERS-1):\n",
        "    encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "        intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        "      )(inputs=encoder_outputs)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=SPA_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "\n",
        "for layer in range(NUM_LAYERS-1):\n",
        "    x = keras_nlp.layers.TransformerDecoder(\n",
        "        intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        "    )(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")\n",
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCFjRfo0XzU6"
      },
      "outputs": [],
      "source": [
        "class WarmupChedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps, s):\n",
        "    self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "    self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float32)\n",
        "    self.s = tf.cast(s, dtype=tf.float32)\n",
        "\n",
        "  def __call__(self, step):\n",
        "     step = tf.cast(step, dtype=tf.float32)\n",
        "     return self.s * tf.math.pow(self.d_model, -0.5) * tf.reduce_min((tf.math.pow(step, -0.5), step * tf.math.pow(self.warmup_steps, -3/2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl2Vm3roKGW4",
        "outputId": "8090eb1c-0248-4627-e6ce-331ecd3c39db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "203/203 [==============================] - 324s 1s/step - loss: 6.3251 - sparse_categorical_accuracy: 0.6912 - val_loss: 2.4277 - val_sparse_categorical_accuracy: 0.7820\n",
            "Epoch 2/12\n",
            "203/203 [==============================] - 237s 1s/step - loss: 1.5313 - sparse_categorical_accuracy: 0.8020 - val_loss: 1.2701 - val_sparse_categorical_accuracy: 0.8159\n",
            "Epoch 3/12\n",
            "203/203 [==============================] - 237s 1s/step - loss: 1.1522 - sparse_categorical_accuracy: 0.8321 - val_loss: 1.0993 - val_sparse_categorical_accuracy: 0.8353\n",
            "Epoch 4/12\n",
            "203/203 [==============================] - 239s 1s/step - loss: 1.0014 - sparse_categorical_accuracy: 0.8509 - val_loss: 0.9501 - val_sparse_categorical_accuracy: 0.8562\n",
            "Epoch 5/12\n",
            "203/203 [==============================] - 238s 1s/step - loss: 0.8648 - sparse_categorical_accuracy: 0.8692 - val_loss: 0.8482 - val_sparse_categorical_accuracy: 0.8694\n",
            "Epoch 6/12\n",
            "203/203 [==============================] - 237s 1s/step - loss: 0.7636 - sparse_categorical_accuracy: 0.8813 - val_loss: 0.7659 - val_sparse_categorical_accuracy: 0.8806\n",
            "Epoch 7/12\n",
            "147/203 [====================>.........] - ETA: 1:05 - loss: 0.6895 - sparse_categorical_accuracy: 0.8898"
          ]
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=keras.optimizers.Adam(WarmupChedule( d_model= EMBED_DIM, warmup_steps=2000, s=1)),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,)\n",
        "transformer.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=12,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w203VZ2g6V-U"
      },
      "outputs": [],
      "source": [
        "def encode(input_sentences):\n",
        "    encoder_input_tokens = tokenizer(input_sentences)\n",
        "    if len(encoder_input_tokens[0]) > MAX_SEQUENCE_LENGTH:\n",
        "        encoder_input_tokens = tf.expand_dims(encoder_input_tokens[0][:MAX_SEQUENCE_LENGTH], 0)\n",
        "\n",
        "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
        "        pads = tf.fill((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
        "        encoder_input_tokens = tf.concat([encoder_input_tokens, pads], 1)\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    encoder_input_tokens = tf.reshape(encoder_input_tokens, (1, MAX_SEQUENCE_LENGTH))\n",
        "    return encoder_input_tokens\n",
        "\n",
        "def decode_sequences(input_sentences):\n",
        "    batch_size = 1\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = tokenizer(input_sentences)\n",
        "    if len(encoder_input_tokens[0]) > MAX_SEQUENCE_LENGTH:\n",
        "        encoder_input_tokens = tf.expand_dims(encoder_input_tokens[0][:MAX_SEQUENCE_LENGTH], 0)\n",
        "\n",
        "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
        "        pads = tf.fill((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
        "        encoder_input_tokens = tf.concat([encoder_input_tokens, pads], 1)\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    encoder_input_tokens = tf.reshape(encoder_input_tokens, (1, MAX_SEQUENCE_LENGTH))\n",
        "    def next(prompt, cache, index):\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens.\n",
        "    length = 40\n",
        "    start = tf.fill((batch_size, 1), tokenizer.token_to_id(\"<s>\"))\n",
        "    pad = tf.fill((batch_size, length - 1), tokenizer.token_to_id(\"<pad>\"))\n",
        "    prompt = tf.concat((start, pad), axis=-1)\n",
        "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        end_token_id=tokenizer.token_to_id(\"</s>\"),\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = generated_tokens\n",
        "    return generated_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRbqjLDl8Zv9"
      },
      "outputs": [],
      "source": [
        "rouge_1 = keras_nlp.metrics.Bleu()\n",
        "\n",
        "for i, test_pair in enumerate(dataset[\"test\"].as_numpy_iterator()):\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "    translated_sentence = tokenizer.detokenize(decode_sequences([input_sentence]))\n",
        "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
        "    translated_sentence = (\n",
        "        translated_sentence.replace(\"<pad>\", \"\")\n",
        "        .replace(\"<s>\", \"\")\n",
        "        .replace(\"</s>\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    rouge_1(tf.expand_dims(tf.convert_to_tensor(reference_sentence), 0), tf.expand_dims(tf.convert_to_tensor(translated_sentence), 0))\n",
        "\n",
        "    if i > 100:\n",
        "      break\n",
        "\n",
        "print(\"BLEU-1 Score: \", rouge_1.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLsID4XNOHsL"
      },
      "outputs": [],
      "source": [
        "reference_sentence, translated_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsHvE_PEKVd_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
